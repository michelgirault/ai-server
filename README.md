introduction: simple lightweight inference server container

1: Llama server, simple inference server for docker based on llama cpp python<br>
2: stable diffusion server, basic server based on diffusers
3: vLLM package
